= Publishing Domain Updates Over Kafka in .NET
Joe Schmetzer <joe.schmetzer@instaclustr.com>;
:backend: docbook
:doctype: book
:docinfo: shared
:xrefstyle: basic
:toc:

== Introduction

Congratulations! You’ve just started a new job as a Technical Lead at Example Ltd. The Engineering Manager introduces you to the team, and gets down to the nuts and bolts of your goals over the next several months.

The situation is that Example Ltd has been growing like crazy for a number of years, and their different line of business systems are all a bit of a complicated mess right now. Each department has independent disconnected views of customer data, and it’s causing big problems. The Sales system has one view of the customer, the Billing system a different view with a different database, while the Operations department has yet another third party system with yet another view. Keeping all these systems in sync is a major challenge, getting exponentially worse as the company grows. Currently data entry clerks manually copy data from one system to another, but this is a slow and error prone process. If the company was able to automate this integration, it would provide up-to-date information in all systems, reduce errors in billing and operations, and free up the time of the people doing the manual updates to do more productive work.

Your goal:

* Design and build an software integration layer that supports keeping the different line of business systems with an up-to-date view of all the customer data
* Follow the company-wide directive on technology use (.NET/C# for implementation language, Apache Kafka for integration)

== Breaking Down the Problem

You sketch out the current state architecture, as described by the engineering manager:

[grid=none,frame=none,options="pgwide"]
|===
a|[pikchr,scaledwidth=100%,title=Current state architecture: stovepipe hell]
....
include::macros.pikchr[]

SalesUser: box invis width 0.5cm height 1cm
user(1cm,"Sales User",.c,SalesUser.c)
SalesSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .w at 1cm right of SalesUser.e
"Sales System" italic with .n at .5cm below previous.n
arrow from SalesUser.e to SalesSystem.w

BillingUser: box invis width 0.5cm height 1cm \
    with .n at 1cm below SalesUser.s
user(1cm,"Billing User",.c,BillingUser.c)
BillingSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .w at 1cm right of BillingUser.e
"Billing System" italic with .n at .5cm below previous.n
arrow from BillingUser.e to BillingSystem.w

OperationsUser: box invis width 0.5cm height 1cm \
    with .n at 1cm below BillingUser.s
user(1cm,"Operations User",.c,OperationsUser.c)
OperationsSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .w at 1cm right of OperationsUser.e
"Operations System" italic with .n at .5cm below previous.n
arrow from OperationsUser.e to OperationsSystem.w

IntegrationUser: box invis width 0.5cm height 1cm \
    with .w at 3cm right of BillingSystem.e
user(1cm,"Integration User",.c,IntegrationUser.c)
spline from SalesSystem.e \
    then to 1cm right of SalesSystem.e \
    then to 1cm left of IntegrationUser.w \
    then to IntegrationUser.w \
    <-
spline from BillingSystem.e \
    then to 1cm right of BillingSystem.e \
    then to 1cm left of IntegrationUser.w \
    then to IntegrationUser.w \
    <- \
    "manual" italic small "updates" italic small
spline from OperationsSystem.e \
    then to 1cm right of OperationsSystem.e \
    then to 1cm left of IntegrationUser.w \
    then to IntegrationUser.w \
    <-
....
|===

Essentially, the three line of business systems are independent stovepipes with their own set of users, with additional back office integration users doing all the legwork of systems integration by manually copying data from one system to another.

Our goal is to break down the stovepipe systems by introducing automated software integrations between systems, and eventually removing the need for the integration user completely.

How can we go about this? One approach is to recognise this scenario is a good place to use event sourcing, where application can publish events to an external append-only event log. Kafka fits the bill here admirably, as an append-only log forms the core abstraction underlying Kafka.

With this high level concept decided, the next incremental steps become evident. For each of the individual stovepipe systems, we need to:

* Model the domain events that will be published
* Implement the code to publish the customer updates to Kafka
* Implement the code to consume the customer updates from Kafka

This is actually a lot of work to do in one step. We can apply the pareto principle: 80% of the value of integration will be gained if we focus on the main sales funnel, where sales users capture details of sales prospects, which are then shared with other systems when the sale is finalised. That is, we will initially look at implementing a one-way integration, with updates flowing from the sales system to the billing system and the operations system. We will defer other integrations to a later date.

Sketching out the future state architecture, it will look something like this:

[grid=none,frame=none,options="pgwide"]
|===
a|[pikchr,scaledwidth=100%,title=Future state architecture: sales updates published and consumed over Kafka]
....
include::macros.pikchr[]

ICMS: [
    Kafka: box width 4cm height 2cm radius 0.125cm fill $colourCluster
    topic("Sales.Customer", .c, 1.25cm below Kafka.n, TopicSalesUpdates)
    "Kafka Cluster" bold with .n at .25cm below Kafka.n
]

SalesSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .ne at 3cm left of ICMS.Kafka.nw
"Sales System" italic with .n at .5cm below previous.n
SalesUser: box invis width 0.5cm height 1cm with .e at 1cm left of SalesSystem.w
user(1cm,"Sales User",.c,SalesUser.c)
arrow from SalesUser.e to SalesSystem.w
spline from SalesSystem.e \
    then to 1cm right of SalesSystem.e \
    then to 0.125cm above 1cm left of ICMS.TopicSalesUpdates.w \
    then to 0.125cm above ICMS.TopicSalesUpdates.w \
    ->
"publish" small italic with .sw at SalesSystem.e

BillingUser: box invis width 0.5cm height 1cm \
    with .n at 1cm below SalesUser.s
user(1cm,"Billing User",.c,BillingUser.c)
BillingSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .w at 1cm right of BillingUser.e
"Billing System" italic with .n at .5cm below previous.n
arrow from BillingUser.e to BillingSystem.w
spline from BillingSystem.e \
    then to 1cm right of BillingSystem.e \
    then to 0.125cm below 1cm left of ICMS.TopicSalesUpdates.w \
    then to 0.125cm below ICMS.TopicSalesUpdates.w \
    <-
"consume" small italic with .nw at BillingSystem.e

OperationsUser: box invis width 0.5cm height 1cm \
    with .n at 1cm below BillingUser.s
user(1cm,"Operations User",.c,OperationsUser.c)
OperationsSystem: box width 3cm height 1.5cm radius 0.125cm \
    with .w at 1cm right of OperationsUser.e
"Operations System" italic with .n at .5cm below previous.n
arrow from OperationsUser.e to OperationsSystem.w
spline from OperationsSystem.e \
    then to 1cm right of OperationsSystem.e \
    then to 0.125cm below 1cm left of ICMS.TopicSalesUpdates.w \
    then to 0.125cm below ICMS.TopicSalesUpdates.w \
    <-
"consume" small italic with .nw at OperationsSystem.e
....
|===

Although not drawn in the diagram, the integration users are still working on updates flowing back into other systems from billing and operations.

== Modelling Domain Events

.Bounded Context
****
As you try to model a larger domain, it gets progressively harder to build a single unified model. Different groups of people will use subtly different vocabularies in different parts of a large organization. The precision of modeling rapidly runs into this, often leading to a lot of confusion. Typically this confusion focuses on the central concepts of the domain.

Domain Driven Design solves this confusion by splitting similar but incompatible domain concepts into different bounded contexts. In this example, there is a sales context, a billing context and an operations context, each with their own definition of exactly what attributes and semantics belong to a "Customer".
****

We need to come up with a customer data model to represent the update events from the sales system. The sales system, the billing system, and the operations system will all have internal representations of exactly what a "Customer" is, but it is almost certain that the fields and semantics of each internal model is subtly different. For those are familiar with Domain Driven Design (DDD), this is called a https://www.martinfowler.com/bliki/BoundedContext.html[Bounded Context].

Speaking to domain experts in the sales organisation, we find out more about their view of what a customer is.

.Sales Customer attributes:
* **Customer Identifier**. A unique identifier for the customer. It is an integer generated by the sales system. It is a required attribute.
* **Name**. The full name of the customer. It can be subdivided into "__title__", "__given name__" and "__family name__". It is a required attribute.
* **Location**. The location of the customer. This can be subdivided into "__city__", "__state__", and "__country__". It is an optional attribute.
* **Phone Number**. The phone number of the customer. It is an optional attribute.
* **Email**. The email address of the customer. It is an optional attribute.
* **Company**. The company that the customer works for. It is subdivided into a "__company id__" and a "__company name__". It is an optional attribute.
* **Lead Source**. The name of person who initially introduced the lead to the sales team. It is an optional attribute.
* **Lead Owner**. The name of the sales person who is managing contact and communication with the customer. It is an optional attribute.

Note that this information is very tuned to the needs of the sales organisation, and would be considered incomplete for the purposes of billing or operations. Optimising for the bounded context of sales greatly simplifies the modelling needs.

The next step in our process is to turn this conceptual domain model into code and data that can be published to Kafka for consumption by other dependant services.

This is the point where a designing and using a __schema__ for the domain model can save a lot of problems. Kafka can very happily take any data passed to it. However, that data is only valuable if the consumer understand the data, too. By clearly defining upfront the fields and acceptable values, and controlling for forwards and backwards compatibility, an organisation can ensure their organisational data remains readable, and provides a smooth migration path well into the future.

https://avro.apache.org/docs/current/[Apache Avro] is a data serialization system built on top of schemas. Using a combination of Apache Avro, the Confluent Schema Registry and the Confluent.Kafka client libraries, we can build a data pipeline to publish events from sales system to Kafka, and then flowing downstream to other consumers.

Our next steps:

* Define a schema for the sales customer
* Generate C# code from the schema to allow easy handling of the data in C# code
* Write code to publish the events to Kafka
* Write code to consume the events from Kafka

== Defining a Schema for Sales Customer

Apache Avro provides two ways of defining a schema:

* **Avro JSON Schema**. The https://avro.apache.org/docs/current/spec.html[Avro Schema Specification] defines how a valid JSON schema can be constructed.
* **Avro IDL Schema**. The Avro IDL (Interface Definition Language) is a more C-like expression of a schema. It is functionally equivalent to the Avro JSON schema, but it can be easier to read for people who are familiar with c-like programming languages. The https://avro.apache.org/docs/current/idl.html[Avro IDL Specification] defines how a valid IDL schema can be constructed.

We are going to use the IDL schema definition. From the previous analysis of a sales customer, we can define the schema as follows:

[grid=rows,frame=all,options="pgwide"]
|===
|sales-customer.avdl

a|[source,avro idl]
----
include::../examples/Example.Domain/avro/sales-customer.avdl[]
----
|===

Given the above IDL, we need to use it to generate the C# code. This is a two-step process. Firstly, the idl file is used to generate Avro JSON schema files. Secondly, the JSON schema files are converted to C# classes that can be used directly.

Avro publishes a Java-based tool to turn ```.avdl``` files to ```.avsc``` files, called ```avro-tools```. It is packaged inside a ```.jar``` file, and can be retrived from the Maven Central repository.

We can download the tool as follows:

```
> mkdir -p $HOME/.avro
> curl --create-dirs \
    --output $HOME/.avro/avro-tools.jar \
    https://repo1.maven.org/maven2/org/apache/avro/avro-tools/1.10.0/avro-tools-1.10.0.jar
```

Once downloaded, we can generate ```.avsc``` files as follows:

```
> mkdir -p build/generated/avro
> java -jar ~/.avro/avro-tools.jar idl2schemata \
    sales-customer.avdl build/generated/avro
```

To convert the ```.avsc``` files to ```.cs``` files, we use the ```avrogen``` tool, which is part of https://www.nuget.org/packages/Apache.Avro.Tools/[https://www.nuget.org/packages/Apache.Avro.Tools].

Install ```avrogen``` using the command:

```
> dotnet tool install --global Apache.Avro.Tools --version 1.10.2
```

Now, we can create the ```.cs``` files as follows:

```
> mkdir -p build/generated/avro
> for f in build/generated/avro/*.avsc; \
    do avrogen -s $f build/generated/cs; \
    done
```

The above steps are complex and hard to remember. Fortunately, with knowledge of scripting, we can create a tool to automate all of this. The script ```avro.sh``` does all the steps above, and can be found at https://github.com/instaclustr/instaclustr-kafka-dotnet/blob/main/examples/schema/avro.sh[https://github.com/instaclustr/instaclustr-kafka-dotnet/blob/main/examples/schema/avro.sh].

To use, run the command as follows:

```
> ./avro.sh generate
Checking avro-tools installation
-> avro-tools already installed at /home/joe/.avro/avro-tools.jar
Generating schema from idl into build/generated/avro
-> Converting avro/sales-customer.avdl
Generating C# from schema into build/generated/csharp
-> Converting build/generated/avro/Company.avsc
-> Converting build/generated/avro/Customer.avsc
-> Converting build/generated/avro/Location.avsc
-> Converting build/generated/avro/PersonName.avsc
```

The messages show the script:

* checking that `avro-tools.jar` is installed
* generating the JSON schema files from the IDL
* generating the C# from the JSON schema

The final step is to add a `.csproj` file to call the script when building. We can hook into the dotnet build steps by adding the following target into the project:

[source,xml]
----
<Target Name="PreBuild" BeforeTargets="PreBuildEvent">
  <Exec WorkingDirectory="$(ProjectDir)" Command="./avro.sh generate" />
  <ItemGroup>
    <Compile Include="$(ProjectDir)build/generated/csharp/**/*.cs" Exclude="@(Compile)" />
  </ItemGroup>
</Target>
----

The full example of this can be found in https://github.com/instaclustr/instaclustr-kafka-dotnet/blob/main/examples/schema/Example.Schema.csproj[https://github.com/instaclustr/instaclustr-kafka-dotnet/blob/main/examples/schema/Example.Schema.csproj]

== Publishing Sales Customer Events into Kafka

Now that we have a schema, and the C# code that we can use to interact with the schema, we're now in a position to actually write events into a Kafka topic.

Before we do anything else, we first need a Kafka cluster to actually write our data. Initially, we can use a single node Kafka cluster running locally to test out our code. This is not a production safe environment, but it gets us up and running.

The following ```docker-compose.yml``` file will set up three services:

* Apache Zookeeper (currently required by Kafka)
* Apache Kafka (running in as a single broker)
* Confluent Schema Registry

[grid=rows,frame=all,options="pgwide"]
|===
|examples/docker/docker-compose.yml

a|[source,yaml]
----
include::../examples/docker/docker-compose.yml[]
----
|===

To start these services, run the command:

[source,bash]
----
> docker-compose up -d
----

The first time you run this will will need to download three large images corresponding to the services. Subsequent runs will be quicker.

Once Kafka is up and running, we can write and test the code to write the sales customer update.

Firstly, how to we create the sales data? In this case, we can create test data by using the ```Faker.Data``` library.

[grid=rows,frame=all,options="pgwide"]
|===
|examples/Example.Producer.Sales/FakeDataGenerator.cs
a|[source,csharp]
----
include::../examples/Example.Producer.Sales/FakeDataGenerator.cs[]
----
|===

With this fake data tool, we're now in a position where we can write sales customer data into the topic. The class ```CustomerUpdateGenerator``` contains a number of functions to do this.

The first function is the ```Run``` method:

[grid=rows,frame=all,options="pgwide"]
|===
|examples/Example.Producer.Sales/CustomerUpdateGenerator.cs - Run
a|[source,csharp]
----
include::../examples/Example.Producer.Sales/CustomerUpdateGenerator.cs[tag=Run,indent=0]
----
|===

This creates a Kafka producer buy calling the ```BuildProducer``` method (see below), and then starting an infinite loop to produduce a fake customer (by calling the ```ProducerFakeCustomer``` method), and then sleeping for one second.

The ```BuildProducer``` method:

[grid=rows,frame=all,options="pgwide"]
|===
|examples/Example.Producer.Sales/ExampleSalesProducer.cs - BuildProducer
a|[source,csharp]
----
include::../examples/Example.Producer.Sales/CustomerUpdateGenerator.cs[tag=BuildProducer,indent=0]
----
|===

This method does the heavy lifting of setting up the configuration of everything needed to create the producer. As we are using Avro formatted messages, we also need to configure the connection to the Schema Registry, and configure the producer to serialise the data using Avro.

In this example, the Schema Registry and the Kafka brokers are all hard-coded to be on the local host. In future tutorials, we will change this to use a "real" Kafka cluster.

The final method is ```ProduceFakeCustomer```:

[grid=rows,frame=all,options="pgwide"]
|===
|examples/Example.Producer.Sales/ExampleSalesProducer.cs - ProduceFakeCustomer
a|[source,csharp]
----
include::../examples/Example.Producer.Sales/CustomerUpdateGenerator.cs[tag=ProduceFakeCustomer,indent=0]
----
|===

This generates a fake customer object (with all the fields filled in with random, fake data). It then uses the producer to send the message to Kafka, and then asynchronously write a message showing the result of the write.

The final piece of the puzzle is to create a ```Main``` method to allow this to be called from the command line:

[grid=rows,frame=all,options="pgwide"]
|===
|examples/Example.Producer.Sales/Program.cs
a|[source,csharp]
----
include::../examples/Example.Producer.Sales/Program.cs[]
----
|===

This simply news up an instance of our ```CustomerUpdateGenerator``` class, and calls ```Run``` on it.

We can run this from the command line as follows:

[source,bash]
----
> dotnet run
Writing customer 8269db07-c6fe-4cc4-a807-b137cf068383 (Willian Ohlund)...
-> Wrote customer 8269db07-c6fe-4cc4-a807-b137cf068383 to [Sales.CustomerUpdates [[2]] @0]
Writing customer 107fa974-e0fa-492b-8fed-5bb4a521857a (Sandee Benbenek)...
-> Wrote customer 107fa974-e0fa-492b-8fed-5bb4a521857a to [Sales.CustomerUpdates [[0]] @0]
Writing customer 14e55c6c-6dd5-45d9-a6c5-c0d4f8ebe3ce (Alice Lada)...
-> Wrote customer 14e55c6c-6dd5-45d9-a6c5-c0d4f8ebe3ce to [Sales.CustomerUpdates [[1]] @0]
Writing customer b7f4a8fe-b0bc-4a96-86d0-ccefb81f5fdd (Joette Irmeger)...
-> Wrote customer b7f4a8fe-b0bc-4a96-86d0-ccefb81f5fdd to [Sales.CustomerUpdates [[0]] @1]
...
----

The program will keep on running until you terminate it by pressing ```<ctrl-c>```.

With that, we have a fully working example that is writing sales customer updates into a Kafka topic.

Next, we will explore building a program to read and use that data.